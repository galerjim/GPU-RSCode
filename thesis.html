<!doctype html>
<html lang="en">

<head>
<meta charset="utf-8">

<title>Accelerate Reed-Solomon on GPUs</title>

<meta name="description" content="Accelerate Reed-Solomon on GPUs">
<meta name="author" content="Shuai YUAN">

<meta name="apple-mobile-web-app-capable" content="yes" />
<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />

<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

<link rel="stylesheet" href="css/reveal.css">
<link rel="stylesheet" href="css/custom.css">
<link rel="stylesheet" href="css/theme/serif.css" id="theme">

<!-- For syntax highlighting -->
<link rel="stylesheet" href="lib/css/zenburn.css">

<!-- For displaying theorems in LaTeX ways, 
moved to custom.css.
<link rel="stylesheet" href="css/theorems.css">
-->

<!-- For printing -->
<!--
		<link rel="stylesheet" href="css/print/pdf.css">
		-->

<!-- If the query includes 'print-pdf', include the PDF print sheet -->
<script>
if( window.location.search.match( /print-pdf/gi ) ) {
  var link = document.createElement( 'link' );
  link.rel = 'stylesheet';
  link.type = 'text/css';
  link.href = 'css/print/pdf.css';
  document.getElementsByTagName( 'head' )[0].appendChild( link );
}
</script>
<!--[if lt IE 9]>
		<script src="lib/js/html5shiv.js"></script>
		<![endif]-->
</head>

<body>

<div class="reveal">

  <!-- Any section element inside of this container is displayed as a slide -->
  <div class="slides">
	<section>
	  <h2>Accelerate Reed-Solomon Codes on GPUs</h2>
	  <br>
	  <h3>
		Student: <a href="http://yszheda.github.io">Shuai Yuan</a>, 
		<br>Advisor: Prof. Jerry Chi-Yuan Chou
		<br>
		<a href="lsalab.cs.nthu.edu.tw/home">LSA Lab, NTHU</a>
	  </h3>
	  <aside class="notes">
		各位口试委员、老师、同学，大家好。
		我是LSA实验室的袁帅，我的指导教授是周志远老师。
		我的thesis题目是应用GPU加速Reed-Solomon Codes的编解码。
	  </aside>
	</section>

	<section>
	  <h2>Outline</h2>
	  <ul>
		<li>Introduction</li>
		<li>Overview of Our Acceleration Targets</li>
		<li>Accelerating Operations in Galois Field</li>
		<li>Accelerating Matrix Multiplication</li>
		<li>Reducing Data Transfer Overhead</li>
		<li>Experiment</li>
		<li>Conclusion</li>
	  </ul>
	  <aside class="notes">
		这是我这次的outline。
		首先我会介绍这个题目的background和motivation。
		接下来我会讲述我们要针对哪些方面进行加速。
		然后对于这里所列出的三个要被加速的目标，我会进一步介绍我们用了哪些方法来优化。
		最后是实验和conclusion。
	  </aside>
	</section>

	<!-- Example of nested vertical slides -->
	<section>
	  <section>
		<h2>Introduction</h2>
		<aside class="notes">
		  首先介绍这个题目的background与motivation。
		</aside>
	  </section>
	  <section>
		<h3>Why Redundancy?</h3>
		<ul>
		  <li>
		  Cloud storage vendors claim to provide highly <em>available</em> and <em>reliable</em> services in their SLA (Service-Level Agreement) with the customers.
		  Both availability and reliability imply strict <em>fault tolerance</em> requirements for cloud storage system.
		  </li>
		  <li>
		  However, as the scale of storage system grows larger and larger, the probability of failure becomes significant:
		  <!--
		  <br>
		  If the MTTF (Mean Time To Failure) of one storage node is $P$, then the MTTF of a system of $n$ storage nodes is $\dfrac{P}{n}$.
			-->
		  <br><p>→ SLA violation.</p>
		  </li>
		</ul>
		<p>
		Therefore, redundancy must be introduced into cloud system.
		</p>
		<aside class="notes">
		  availability和stability是cloud storage vendor与客户之间签署的SLA（Service Level Agreement）中的重要考量。
		  不管是availability还是stability，都对cloud storage system的fault tolerance容错性提出了严格的要求。
		  但是，随着cloud storage system的有越来越多的node，发生failure的机率也越来越大。
		  为了保证不违反SLA，我们需要引入redundancy来保证系统的fault tolerance容错性。
		</aside>
	  </section>
	  <section>
		<h3>Why Redundancy?</h3>
		<ul>
		  <li>
		  The simplest and straightforward redundancy solution is <em>replication</em> of the data in multiple storage nodes.
		  </li>
		  <li>
		  Triple replication solution have been favored in cloud storage systems like the GFS (Google File System) and HDFS (Hadoop Distributed File System).
		  </li>
		</ul>
		<aside class="notes">
		  最简单的redundancy solution是replication。它把data拷贝若干份，再把这若干份分别分散在不同的storage node中。
		  假如其中某个storage node发生failure，从其他的node仍可以拿到原来的data。
		  这种solution也被广泛应用于一些cloud storage system中，例如GFS和HDFS就应用了triple replication（把同一块data存三份）。
		</aside>
	  </section>
	  <section>
		<h3>Why Erasure Codes?</h3>
		<ul>
		  <li>
		  Problem of replication: <em>large storage overhead</em>.
		  </li>
		  <li>
		  Erasure codes can reduce the storage overhead significantly while at the same time maintaining the same level of fault tolerance as replication → a better redundancy solution.
		  </li>
		</ul>
		<aside class="notes">
		  replication的缺陷是有着很高的storage overhead，也就是需要很多额外的存储空间来存放对data的拷贝。
		  而erasure codes在保证相同的fault tolerance level的前提下，可以显著降低这一storage overhead，这也使它成为比replication更好的redundancy solution。
		</aside>
	  </section>
	  <section>
		<h3>Why Erasure Codes?</h3>
		<p>
		$(n, k)$ MDS (Maximum Distance Separable) codes where $n$ and $k$ are integers and $n > k$:
		<ul>
		  <li>File → $k$ equal-size native data chunks.</li>
		  <li>$k$ equal-size native chunks → $(n - k)$ code chunks.</li>
		  <li>The native and code chunks are distributed on $n$ different storage nodes.</li>
		  <li>tolerates the failure of any $(n - k)$ storage nodes.</li>
		</ul>
		</p>
		<p class="fragment">Example:</p>
		<table class="fragment">
		  <tr align="center">
			<td>
			  <img src="./images/dot-graph/e-c.png" alt="erasure code">
			</td>
			<td>
			  <img src="./images/dot-graph/replica.png" alt="replication">
			</td>
		  </tr>
		  <tr align="center">
			<td>
			  Reed-Solomon Code $(n=4, k=2)$
			</td>
			<td>
			  Triple Replication
			</td>
		  </tr>
		</table>
		<p class="fragment">
		Save 50% space!
		</p>
		<aside class="notes">
		  在我们的thesis中，我们所讨论的erasure codes是指(n,k) MDS codes。
		  这一类codes首先会将file平均分为若干native chunks，native chunks的数目为k。
		  之后，k个native chunks通过encode生成(n-k)个code chunks。
		  native chunks和code chunks再被分散到n个不同的storage nodes中。
		  (n,k) MDS codes可以容许高达(n-k)个任意的storage nodes发生failure。
		  (next)
		  下面我们通过一个例子来比较erasure codes和replication。
		  左边是采用(4,2) MDS code，把file平均分为A、B两个native chunks，再通过encoding生成两个绿色方框所表示的code chunks。
		  右边是采用triple replication的结果。
		  (next)
		  这二者都能容许任意两个storage nodes发生failure，但显然erasure codes更胜一筹，它可以节省50%的额外空间。
		</aside>
	  </section>
	  <section>
		<h3>Why Reed-Solomon Codes?</h3>
		<ul>
		  <li>
		  Reed-Solomon codes $RS(k, n-k)$ are one of the most popular MDS erasure codes.
		  </li>
		  <li>
		  $RS(10, 4)$ is used in HDFS-RAID in Facebook and $RS(6, 3)$ is used in GFS II in Google.
		  </li>
		</ul>
		<aside class="notes">
		  在MDS erasure codes当中，Reed-Solomon codes是使用最为广泛的code之一，例如Fackbook和Google的cloud storage system就使用了不同setting的Reed-Solomon codes作为它们的redundancy solution。
		</aside>
	  </section>
	  <section>
		<h3>Shortcomings of Reed-Solomon Codes</h3>
		<ul>
		  <li>
		  <em>extra high computation cost</em> compared to replication: encoding and decoding.
		  </li>
		  <li class="fragment">
		  Our contributions: use GPU to accelerate the Reed-Solomon encoding and decoding.
		  </li>
		</ul>
		<aside class="notes">
		  然而，Reed-Solomon codes虽然有效地降低了storage overhead，但相比replication却需要encoding与decoding，这也造成了额外的high computation cost。
		  为了弥补Reed-Solomon codes的这一缺陷，我们采用GPU作为accelerator来加速它的encoding与decoding，这也是我们这篇thesis的主要贡献。
		</aside>
	  </section>
	</section>

	<section>
	  <section>
		<h2>Overview of Our Acceleration Targets</h2>
		<aside class="notes">
		  下面介绍我们针对哪些方面来对Reed-Solomon codes进行加速。
		</aside>
	  </section>
	  <section>
		<h3>Reed-Solomon Code Overview</h3>
		<table>
		  <tr align="center">
			<td>
			  <img src="./images/dot-graph/encode.png" alt="RS encode">
			</td>
			<td>
			  <img src="./images/dot-graph/decode.png" alt="RS decode">
			</td>
		  </tr>
		  <tr align="center">
			<td>
			  Reed-Solomon Encoding
			</td>
			<td>
			  Reed-Solomon Decoding
			</td>
		  </tr>
		</table>
		<aside class="notes">
		  首先，为了了解Reed-Solomon codes为何有很高的computation cost，我们先来看encoding和decoding的大致过程。
		  encoding的大致过程如左图：n与k被用来生成encoding matrix，encoding matrix再跟k个data chunks组成的matrix进行矩阵乘法，最后得到(n-k)个code chunks。
		  decoding的大致过程如右图：首先从k个没有发生failure的storage nodes中得到k个chunks，并生成decoding matrix，decoding matrix再跟k个chunks组成的matrix进行矩阵乘法，最后得到原来的data。
		  可以看到，无论是encoding还是decoding，其主要过程都是矩阵乘法，而这也正是Reed-Solomon codes的computation bottleneck。
		</aside>
	  </section>
	  <!-- Overview of Three Kinds of Implementation -->
	  <section>
		<h3>Acceleration Targets - 1</h3>
		<p>Computation bottleneck: matrix multiplication.</p>
		\begin{array}{rl}
		& C = A \cdot B \\
		\equiv & ( c_{j} = \sum_{i = 1}^{k} a_{i,j} \times b_{i} ) 
		\end{array}
		<p>Addition and multiplication are defined as arithmetic over Galois Field GF($2^8$).</p>
		<p class="fragment">Acceleration targets for computation:</p>
		<ul>
		  <li class="fragment">Arithmetic over Galois Field.</li>
		  <li class="fragment">Matrix multiplication.</li>
		</ul>
		<aside class="notes">
		  下面让我们来了解矩阵乘法这一computation overhead。
		  矩阵乘法的一般形式是C=AB，如果把它改写成等价的矩阵元素的形式，我们会发现是由一系列加法和乘法所组成。
		  在Reed-Solomon codes的encoding和decoding中，这些加法和乘法并非是我们熟知的实数域的加法和乘法，而是Galois Field的加法和乘法。
		  这个域的四则运算有自己一套法则，它也是造成Reed-Solomon codes的encoding和decoding的computation complexity高的原因之一。
		  (next)
		  所以，针对这些特点，我们需要从微观上加速Galois Field的加法和乘法，从宏观上加速矩阵的乘法。
		</aside>
	  </section>
	  <section>
		<h3>GPU-Accelerated Reed-Solomon Code Overview</h3>
		<table>
		  <tr align="center">
			<td>
			  <small>Reed-Solomon Encoding in a GPU</small>
			</td>
			<td>
			  <img src="./images/dot-graph/GPUencode.png" alt="RS encode in GPU">
			</td>
		  </tr>
		  <tr align="center">
			<td>
			  <small>Reed-Solomon Decoding in a GPU</small>
			</td>
			<td>
			  <img src="./images/dot-graph/GPUdecode.png" alt="RS decode in GPU">
			</td>
		  </tr>
		</table>
		<aside class="notes">
		  然而，使用GPU来加速计算本身也会产生一些问题。
		  从这两张使用GPU来加速Reed-Solomon Codes的encoding与decoding的过程图我们可以看到，由于CPU和GPU有不同的virtual address，使得计算的input和结果需要在二者的memory之间被transfer。
		</aside>
	  </section>
	  <section>
		<h3>Acceleration Targets - 2</h3>
		<p>Extra overhead in GPU implementation: data transfers between CPU and GPU.</p>
		<p class="fragment">Another acceleration targets: reducing data transfer overhead.</p>
		<aside class="notes">
		  (next)
		  因此，我们除了在GPU中加速计算过程之外，还需要减少CPU与GPU之间额外的data transfer overhead。
		</aside>
	  </section>
	  <section>
		<h2>Overview of Our Acceleration Targets</h2>
		<ul>
		  <li>Accelerating arithmetic over Galois Field.</li>
		  <li>Accelerating matrix multiplication.</li>
		  <li>Reducing data transfer overhead.</li>
		</ul>
		<aside class="notes">
		  以上就是我们用GPU加速Reed-Solomon codes编解码的三大方面，下面的sessions将一一针对这些方面进行介绍。
		</aside>
	  </section>
	</section>

	<section>
	  <section>
		<h2>Accelerating Arithmetic over Galois Field</h2>
		<aside class="notes">
		  首先介绍如何加速Galois Field的运算。
		</aside>
	  </section>
	  <section>
		<h3>Breif Introduction of Galois Field</h3>
		<p>GF($2^w$) contains $2^w$ polynomials. For each polynomial, its degree is at most $w−1$ and its coefficient is in {0, 1}.</p>
		<p align="left">For GF($2^8$), every element can be one-to-one mapped into a byte, and polynomial operations in GF($2^8$) is isomorphic to operations on bytes:</p>
		<ul>
		  <li>Addition: isomorphic to bitwise XOR → inexpensive.</li>
		  <li>
		  Multiplication: isomorphic to bitwise operations → still time-consuming.
		  <!--
		  <br>cost a loop of at most eight iterations to multiply two elements in GF($2^8$) → called as <em>loop-based</em> 
			-->
		  </li>
		</ul>
		<p class="fragment">
		Therefore, how to accelerate <em>multiplication</em> over GF($2^8$) will be our focus.
		</p>
		<aside class="notes">
		  这里对Galois Field做一个粗略的介绍。
		  Galois Field其实是多项式域。GF(2^w)表明它有2^w个多项式元素,。每个多项式的次数不超过w-1，系数非0即1。
		  我们之前所看到的GF(2^8)这个域有一个性质，每个元素可以被一一对应到某个byte，而这个域的运算也可以被同构到针对byte的运算。
		  其中，加法被同构到bitwise XOR，它的computation cost很低。而乘法所同构到的一系列运算仍然是time-consuming的。
		  (next)
		  因此，如何加速乘法运算是我们接下来的重点。
		</aside>
	  </section>
	  <section>
		<h3>GPU Accelerating Options for Multiplication</h3>
		<ul>
		  <li><em>loop-based</em> method</li>
		  <li>a set of <em>table-based</em> methods</li>
		</ul>
		<aside class="notes">
		  而针对Galois Field乘法的加速方法主要有两大类，一种是loop-based，另一类是一系列的table-based方法。
		  以下我们将探讨在GPU中应当选择哪一种方法。
		</aside>
	  </section>
	  <section>
		<h4>Loop-based Method</h4>
		<ul>
		  <li>compute directly: cost a loop of at most eight iterations to multiply two elements in GF($2^8$).</li>
		  <li>computation bound</li>
		</ul>
		<aside class="notes">
		  loop-based的方法就是直接用同构得到的一系列bitwise operations来做计算。
		  由于计算两个元素的乘法需要跑完一个loop，故而得名。
		  在计算GF(2^8)的乘法时，loop-based需要至多多达八次的iterations。
		  显而易见，它是computation bound的。
		</aside>
	  </section>
	  <section>
		<h4>Table-based Methods</h4>
		<table>
		  <tr>
			<td></td>
			<td>Full Multiplication Table</td>
			<td>"Double Table"/"Left-Right Table"</td>
			<td>Log&Exp Table</td>
		  </tr>
		  <tr>
			<td>Space Complexity for GF($2^w$)</td>
			<!--<td>$O(2^w \times 2^w)$</td>-->
			<td>$O(2 ^ { 2w })$</td>
			<td>$O(2^{3w/2+1})$</td>
			<td>$O( 2^{w+1} )$</td>
		  </tr>
		  <tr>
			<td>Computation Complexity</td>
			<td>one table-lookup</td>
			<td>2 table-lookup, 2 AND, 1 XOR, and 1 SHIFT</td>
			<td>3 table-lookup, 1 <em>MOD</em>, 1 ADD, and 2 <em>branches</em></td>
		  </tr>
		  <tr class="fragment">
			<td>Memory space for GF($2^8$)</td>
			<td>64 KB</td>
			<td>8 KB</td>
			<td>512 Bytes</td>
		  </tr>
		</table>
		<p class="fragment">
		Use log&exp table-based method in our GPU implementation.
		</p>
		<aside class="notes">
		  相较于loop-based，table-based由于采用先计算完结果再查表的方式，因此computation cost大为降低。
		  table-based方法主要分为三种，从左到右，所需要的空间依次减少，而computation cost却也依次增加（例如最右边的log&exp table方法，需要MOD运算和conditional branches作为判断条件，cost比较高）。
		  (next)
		  在GPU的实作中，我们需要table的size越小越好，以便能装入cache中。在这里，log&exp table方法是最合适的。
		</aside>
	  </section>
	  <section>
		<h4>GPU Implementation: Loop-based or Table-based?</h4>
		<img src="./images/exp/table-based vs. loop-based/chunk-size-scaling-k1M_64M/encode-BW.png" width="60%" height="60%" alt="Loop-Based vs Table-Based">
		<ul>
		  <li>The loop-based method is able to achieve the maximum bandwidth even when the chunk size is small.</li>
		  <li>The bandwidth of the table-based method can still scale up as the chunk size grows larger.</li>
		  <li>The maximum bandwidth of the table-based method exceeds that of the loop-based method.</li>
		</ul>
		<aside class="notes">
		  下面我们通过一个实验来决定采用loop-based或是table-based。
		  在我们的实验中，chunk size从1MB scale到64MB，k从4 scale到16，采用double fault tolerance (code chunks的数目是二)。
		  从图中可以看出（较平的三条线是loop-based，其他的是table-based）：
		  Loop-based在chunk size很小的时候performance优于table-based。
		  但是table-based的effective bandwidth仍然可以随着chunk size变大继续scale，而loop-based却早已达到它的最高bandwidth。
		  最后从最大bandwidth来看，table-based要优于loop-based。

		  之前我们survey时看到，一些paper在优化其他一些erasure codes时采用了loop-based。
		</aside>
	  </section>

	  <section>
		<h3>Further Improvement of the Log&exp Table-based Method</h3>
		<aside class="notes">
		  决定了GPU实作采用log&exp table-based方法之后，我们再继续对这一方法进行优化。
		</aside>
	  </section>
	  <section>
		<h3>Further Improvement of the Log&exp Table-based Method</h3>
		<table>
		  <tr>
			<td>Improvement Approach 1</td>
			<td>Replace the slow modular operations with more efficient operations.</td>
		  </tr>
		  <tr>
			<td>Improvement Approach 2</td>
			<td>Remove the slow modular operations by augmenting one table.</td>
		  </tr>
		  <tr>
			<td>Improvement Approach 3</td>
			<td>Further eliminates the conditional branches by augmenting both two tables.</td>
		  </tr>
		</table>
		<aside class="notes">
		  我们survey时看到，一些paper提出了这三种优化方法。
		  这些方法优化的目标是之前我们所提到的MOD和branches。
		  前两种优化主要是针对MOD这一种耗时的operation，第一种是把它替换成等价的operations，第二种是通过扩充table的内容来去掉MOD。
		  第三种优化除了去掉MOD以外，还去掉了branches，它的方式也是对table的内容进行扩充。
		</aside>
	  </section>
	  <section>
		<h3>Further Improvement of the Log&exp Table-based Method</h3>
		<p>
		In GPU implementation, where to store the tables and how to initialize them will affect the performance.
		</p>
		<img src="./images/memory-spaces-on-cuda-device.png" width="50%" height="50%" alt="Loop-Based vs Table-Based">
		<p class="fragment">
		Appropriate GPU memory:
		</p>
		<ul class="fragment">
		  <li>constant memory: off-chip memory whose accesses are usually cached in the constant cache.</li>
		  <li>shared memory: on-chip memory which has the smallest access latency except the register file.</li>
		</ul>
		<aside class="notes">
		  在GPU的实作中，还有一个新的问题，就是如何存放tables，还有如何initialize tables。
		  我们先来看tables的存放。这张图给出了GPU各种memory space。
		  其中左边深蓝色的方框是DRAM，代表这块memory是off-chip的，latency大概是400-800个clock cycles。
		  在off-chip memory中，我们考虑采用constant memory，因为它的access会被cache在constant cache中。
		  而另一个考虑采用的memory是shared memory，它位于右边multiprocessor的黄色方框中，有着仅次于register的latency。
		  （根据我们跑一些micro-benchmarking的结果，我们的卡是48cycles）。
		</aside>
	  </section>
	  <section>
		<h3>Further Improvement of the Log&exp Table-based Method</h3>
		<p>
		What we have implemented:
		</p>
		<ul class="fragment">
		  <li>Store two tables in the constant memory and initialize them at compile time.</li>
		  <li>Store two tables in the shared memory and run-time initialize them serially at the beginning of each kernel function.</li>
		  <li>Store two tables in the off-chip memory and then load into the shared memory parallely at the beginning of each kernel function.</li>
		</ul>
		<aside class="notes">
		</aside>
	  </section>
	  <section>
		<h4>Further Improvement of the Log&exp Table-based Method</h4>
		<p>
		<small>encoding a 1GB file with $k = 4$ and $n = 6$.</small>
		</p>
		<img src="./images/exp/log&exp table/high-occupancy/sMem-vs-cMem.png" width="60%" height="60%" alt="Log&exp improvement techniques">
		<ul>
		  <li>The elimination of conditional branches improves the performance, for it removes warp diverge.</li>
		  <li>Accessing the tables in the constant memory is more time-consuming.</li>
		</ul>
	  </section>
	</section>

	<section>
	  <section>
		<h2>Accelerating Matrix Multiplication</h2>
	  </section>
	  <section>
		<h3>Naive Implementation</h3>
		<img src="./images/matrix-multiplication/without-tiling.png" width="60%" height="60%" alt="without tiling">
		<aside class="notes">
		  如图所示，要计算某行某列（红色方格）的结果，需要矩阵A红色的row和矩阵B红色的column。
		</aside>
	  </section>
	  <section>
		<h3>Naive Implementation</h3>
		<img src="./images/matrix-multiplication/without-tiling-1.png" width="60%" height="60%" alt="without tiling">
		<aside class="notes">
		  接下来，我们拿矩阵A红色的row中第一个element和矩阵B红色的column中第一个element相乘得到一个结果。
		</aside>
	  </section>
	  <section>
		<h3>Naive Implementation</h3>
		<img src="./images/matrix-multiplication/without-tiling-2.png" width="60%" height="60%" alt="without tiling">
		<aside class="notes">
		  再拿第二个element相乘，其结果与上一个结果相加。
		</aside>
	  </section>
	  <section>
		<h3>Naive Implementation</h3>
		<img src="./images/matrix-multiplication/without-tiling-3.png" width="60%" height="60%" alt="without tiling">
		<aside class="notes">
		  以此类推，计算出矩阵C中红色方格的值。
		</aside>
	  </section>
	  <section>
		<h3>Problems of Naive Implementation</h3>
		<img src="./images/matrix-multiplication/without-tiling.png" width="40%" height="40%" alt="without tiling">
		<ul>
		  <li>a lot of global memory (off-chip) transactions: each takes 400 - 800 clock cycles.</li>
		  <li>memory access in column major → poor temporal locality and high cache missrate.</li>
		</ul>
	  </section>
	  <section>
		<h3>Square-Tiling Algorithm</h3>
		<img src="./images/matrix-multiplication/square-tiling.png" width="60%" height="60%" alt="square tiling">
		<aside class="notes">
		  如图，左边与上边被虚线框起来的黄色正方形称为tile。
		  tile中的elements会从global memory中被load进shared memory，之后在计算中就直接在shared memory中进行access。
		  正如之前所讲的，shared memory是一层programmable cache，latency大概在20-40个cycles（实测值约48）。
		</aside>
	  </section>
	  <section>
		<h3>Problems of Square-Tiling Algorithm</h3>
		<p>not suitable for general input cases of Reed-Solomon Codes: <br>a small matrix multiple a huge matrix</p>
		<ul>
		  <li>encoding--A: $(n - k) \times k$, B: $k \times CS$.</li>
		  <li>decoding--A: $k \times k$, B: $k \times CS$.</li>
		</ul>
		<p>where</p>
		<ul>
		  <li>$n$: total chunk number (1-100)</li>
		  <li>$k$: native chunk number (1-100)</li>
		  <li>$CS$: chunk size (more than 10,000,000)</li>
		</ul>
	  </section>
	  <section>
		<h3>Generalized Tiling Algorithm</h3>
		<img src="./images/matrix-multiplication/tiling.png" width="80%" height="50%" alt="generalized tiling">
	  </section>
	  <section>
		<h4>How to Determine the Parameter of Tiles</h4>
		<ul>
		  <li class="fragment">set tileDepth to $k$ → remove the loop of accessing matrix $A$ and $B$ tile by tile.</li>
		  <!--
		  <li class="fragment">assign each thread compute one element of the product tile in matrix $C$ → the CUDA block size is equal to $\textrm{tileWidthRow} \times \textrm{tileWidthCol}$ → find the best CUDA block size by tuning occupancy</li>
			-->
		  <li class="fragment">$\textrm{tileWidthRow} \times \textrm{tileWidthCol}$ is equal to the CUDA block size (number of threads in a CUDA block) → find the best CUDA block size by tuning occupancy.</li>
		  <li class="fragment">Finally we need to further determine tileWidthRow and tileWidthCol.</li>
		</ul>
	  </section>
	  <section>
		<h4>How to Determine the Parameter of Tiles</h4>
		<p>
		Define the aspect ratio $AR$ of the tile in the result matrix as:
		$$
		AR = \dfrac{ \textrm{tileWidthRow} }{ \textrm{tileWidthCol} }
		$$
		</p>
		<p class="fragment">three strategies:</p>
		<table class="fragment">
		  <tr>
			<td>Min $AR$</td>
			<td>minimize tileWidthRow, <br>maximize tileWidthCol</td>
		  </tr>
		  <tr>
			<td>$AR=1$</td>
			<td>$\textrm{tileWidthRow} = \textrm{tileWidthCol}$</td>
		  </tr>
		  <tr>
			<td>Max $AR$</td>
			<td>maximize tileWidthRow, <br>minimize tileWidthCol</td>
		  </tr>
		</table>
	  </section>
	  <section>
		<!--
		<h3>Accelerating Matrix Multiplication over Galois Field</h3>
		-->
		<h4>How to Determine the Parameter of Tiles</h4>
		<p>
		use the following testcases for encoding:
		</p>
		<small>
		  <table>
			<thead>
			  <tr>
				<th>testcase</th>
				<th>$k$</th>
				<th>$n$</th>
				<th>chunk size (MB)</th>
				<th>Description</th>
			  </tr>
			</thead>
			<tbody>
			<tr>
			  <td>1</td>
			  <td>2</td>
			  <td>130</td>
			  <td>1</td>
			  <td>Small file, large code chunk number</td>
			</tr>
			<tr>
			  <td>2</td>
			  <td>4</td>
			  <td>150</td>
			  <td>1</td>
			  <td>Small file, large code chunk number</td>
			</tr>
			<tr>
			  <td>3</td>
			  <td>4</td>
			  <td>200</td>
			  <td>1</td>
			  <td>Small file, large code chunk number</td>
			</tr>
			<tr>
			  <td>4</td>
			  <td>2</td>
			  <td>130</td>
			  <td>10</td>
			  <td>Big file, large code chunk number</td>
			</tr>
			<tr>
			  <td>5</td>
			  <td>4</td>
			  <td>150</td>
			  <td>10</td>
			  <td>Big file, large code chunk number</td>
			</tr>
			<tr>
			  <td>6</td>
			  <td>4</td>
			  <td>200</td>
			  <td>10</td>
			  <td>Big file, large code chunk number</td>
			</tr>
			<tr>
			  <td>7</td>
			  <td>4</td>
			  <td>6</td>
			  <td>256</td>
			  <td>Big file, small code chunk number</td>
			</tr>
			</tbody>
		  </table>
		</small>
		<!--
		<ul>
		  <li>
		  $k = 4$, $n = 6$, chunk size = 256 MB
		  </li>
		  <li>
		  $k = 32$, $n = 64$, chunk size = 16 MB
		  </li>
		  <li>
		  $k = 4$, $n = 132$, chunk size = 3968 Bytes
		  </li>
		  <li>
		  $k = 8$, $n = 10$, chunk size = 64 MB
		  </li>
		  <li>
		  $k = 16$, $n = 18$, chunk size = 16 MB
		  </li>
		  <li>
		  $k = 128$, $n = 130$, chunk size = 2 KB
		  </li>
		  <li>
		  $k = 32$, $n = 34$, chunk size = 8 MB
		  </li>
		  <li>
		  $k = 2$, $n = 4$, chunk size = 4 MB
		  </li>
		  <li>
		  $k = 2$, $n = 4$, chunk size = 1 KB
		  </li>
		  <li>
		  $k = 16$, $n = 20$, chunk size = 16 MB
		  </li>
		</ul>
		-->
	  </section>
	  <section>
		<!--
		<h3>Accelerating Matrix Multiplication over Galois Field</h3>
		-->
		<h4>How to Determine the Parameter of Tiles</h4>
		<img src="./images/exp/tiling strategies/testcases-V3/2TB-tiling-strategies.png" width="60%" height="60%" alt="tiling strategies">
		<ul>
		  <li>Generally, the "Min $AR$" strategy has the least number of conditional branches, while the "Max $AR$" has the most. The overhead of branches is significant when encoding a large file into a large number of code chunks.</li>
		  <li>The ``AR=1'' strategy has significant performance degradation when the number of code chunks is smaller than tileWidthRow (wasting cache space, extra boundary checking).</li>
		</ul>
		<p class="fragment">
		Use the "Min $AR$" strategy.
		</p>
	  </section>
	  <section>
		<h3>Further Improvement of Tiling Algorithm</h3>
		<p>
		When each chunk can be word-aligned, we can further improve the tiling algorithm.
		</p>
	  </section>
	  <section>
		<h4>Observation 1: Global Memory Transactions</h4>
		<img src="./images/matrix-multiplication/tiling-load-1.png" width="80%" height="50%" alt="observation 1">
	  </section>
	  <section>
		<h4>Observation 1: Global Memory Transactions</h4>
		<img src="./images/matrix-multiplication/tiling-load-2.png" width="80%" height="50%" alt="observation 1">
	  </section>
	  <section>
		<h4>Observation 1: Global Memory Transactions</h4>
		<img src="./images/matrix-multiplication/tiling-load-3.png" width="80%" height="50%" alt="observation 1">
		<p>a lot of 8-bit global memory transactions!</p>
	  </section>
	  <section>
		<h4>Improvement Technique 1: Packing Memory Transactions</h4>
		<img src="./images/matrix-multiplication/tiling-align-load-1.png" width="80%" height="50%" alt="observation 1">
	  </section>
	  <section>
		<h4>Improvement Technique 1: Packing Memory Transactions</h4>
		<img src="./images/matrix-multiplication/tiling-align-load-2.png" width="80%" height="50%" alt="observation 1">
	  </section>
	  <section>
		<h4>Improvement Technique 1: Packing Memory Transactions</h4>
		<img src="./images/matrix-multiplication/tiling-align-load-3.png" width="80%" height="50%" alt="observation 1">
		<p>pack 8-bit global memory transactions into 32-bit transactions → reduce global memory load and store.</p>
	  </section>
	  <section>
		<h4>Observation 2: Byte-length Matrix Multiplication</h4>
		<img src="./images/matrix-multiplication/tiling-ALU-1.png" width="80%" height="50%" alt="tiling alignment">
		<ul>
		  <li><del>Multiplication: table look-ups</del></li>
		  <li>Addition: 8-bit addition in 32-bit ALU!</li>
		</ul>
	  </section>
	  <section>
		<h4>Improvement Technique 2: Byte-by-word Matrix Multiplication</h4>
		<img src="./images/matrix-multiplication/tiling-align.png" width="80%" height="50%" alt="tiling alignment">
	  </section>
	  <section>
		<h4>Improvement Technique 2: Byte-by-word Matrix Multiplication</h4>
		<img src="./images/matrix-multiplication/tiling-align-1.png" width="80%" height="50%" alt="tiling alignment">
	  </section>
	  <section>
		<h4>Improvement Technique 2: Byte-by-word Matrix Multiplication</h4>
		<img src="./images/matrix-multiplication/tiling-align-2.png" width="80%" height="50%" alt="tiling alignment">
	  </section>
	  <section>
		<h4>Improvement Technique 2: Byte-by-word Matrix Multiplication</h4>
		<img src="./images/matrix-multiplication/tiling-align-3.png" width="80%" height="50%" alt="tiling alignment">
	  </section>
	  <section>
		<h4>Improvement Technique 2: Byte-by-word Matrix Multiplication</h4>
		<img src="./images/matrix-multiplication/tiling-align-4.png" width="80%" height="50%" alt="tiling alignment">
	  </section>
	  <section>
		<h4>Improvement Technique 2: Byte-by-word Matrix Multiplication</h4>
		<img src="./images/matrix-multiplication/tiling-align-5.png" width="80%" height="50%" alt="observation 2">
	  </section>
	  <section>
		<h3>Further Improvement of Tiling Algorithm</h3>
		<img src="./images/exp/tiling strategies/align/2TB-tiling-strategies.png" width="60%" height="60%" alt="tiling alignment">
		<p>performance improvement: more than 38%</p>
	  </section>
	  <section>
		<h4>Profiler Evaluation: Global Memory Load Transactions</h4>
		<img src="./images/exp/tiling strategies/align/global-load.png" width="60%" height="60%" alt="tiling alignment">
	  </section>
	  <section>
		<h4>Profiler Evaluation: ALU Operations</h4>
		<img src="./images/exp/tiling strategies/align/alu-op.png" width="60%" height="60%" alt="tiling alignment">
	  </section>
	</section>

	<section>
	  <section>
		<h2>Reducing Data Transfer Overhead</h2>
	  </section>
	  <!--
	  <section>
		<h2>Reduce Data Transfer Overhead</h2>
		<ul>
		  <li>Using Pinned Host Memory</li>
		  <li>Using CUDA Streams</li>
		</ul>
	  </section>
-->
	  <section>
		<h3>Using CUDA Streams</h3>
		<p>
		CUDA streams are used for further overlapping data transfers with computation.
		</p>
		<img src="./images/timeline-comparison-for-copy-and-kernel-execution.png" alt="CUDA-stream">
		<br> Sequential vs. Concurrent copy and execute
	  </section>
	  <section>
		<h4>Using CUDA Streams</h4>
		<p>
		encoding under $k = 4, n = 6$ settings.
		</p>
		<p>
		The input file size is scaled from 1000 MB to 2000 MB, and the CUDA stream number is increased from one to four.
		</p>
		<img src="./images/exp/stream/random/k2n3randomStreamSpeedup.png" width="60%" height="60%" alt="streaming speedup">
		<p class="fragment">
		using CUDA streaming can improve the performance by more than 29%.
		</p>
	  </section>
	  <section>
		<h4>Using CUDA Streams</h4>
		<p>
		encoding a 2000 MB file under $k = 4, n = 6$ settings.
		</p>
		<img src="./images/exp/stream/random/k2n3randomStreamTime.png" width="60%" height="60%" alt="streaming time breakdown">
		<p class="fragment">
		The kernel execution time is increasing with the growth of the CUDA stream number. → The overhead of kernel execution will exceed the time saving of data transfer at some point.
		</p>
	  </section>
	</section>

	<section>
	  <section>
		<h2>Experiment</h2>
	  </section>
	  <section>
		<h3>Experiment Setup</h3>
	  </section>
	  <section>
		<h3>Experiment Setup</h3>
		<ul>
		  <li>
		  CentOS-6 with 2.6.32 Linux kernel.
		  </li>
		  <ul>
			<li>Intel Xeon Processor E5-2670 v2 x 2
			<ul>
			  <li>10 cores</li>
			  <li>2.5 GHz</li>
			</ul>
			</li>
			<li>NVIDIA Tesla K20X GPU x 2
			<ul>
			  <li>2688 CUDA cores</li>
			  <li>peak performance: 1.31 Tflops (double precision floating point calculation) and 3.95 Tflops (single precision floating point)</li>
			  <li>maximum size of GPU GDDR5 memory: 6 GB</li>
			  <li>theoretical memory bandwidth 243 GB/s</li>
			  <li>two copy engines → supports concurrent data copy and kernel execution</li>
			</ul>
			</li>
			<li>maximum bidirectional bandwidth of the PCI-Express bus: 8 GB/s</li>
		  </ul>
		</section>
		<section>
		  <h3>Experiment Setup</h3>
		  <ul>
			<li>Input files are randomly generated.</li>
			<li>Most of our experimental results reflect the average of 100 runs.</li>
			<li>Due to the similarity of the performance result of encoding and that of decoding in most experiments, the latter one is omitted.</li>
		  </ul>
		</section>
		<section>
		  <h3>Overall Performance Evaluation</h3>
		  <p>
		  We evaluate the overall performance by encoding a 1600 MB file with $k = 4, n = 6$.
		  </p>
		</section>
		<section>
		  <h3>Overall Performance Evaluation</h3>
		  <h4>Step-by-step Improvement</h4>
		  <img src="./images/exp/step-perf/stepBreakdown.png" width="60%" height="60%" alt="Step-by-step Improvement">
		</section>
		<section>
		  <h3>Overall Performance Evaluation</h3>
		  <h4>GPU vs. CPU</h4>
		  <ul>
			<li>best CPU implementation (Jerasure library, compiled by clang with the -O3 compiler optimization flag): 4309.08 ms.</li>
			<li>optimized GPU implementation: 292.977 ms (14.71x speedup).</li>
		  </ul>
		</section>
	  </section>

	  <section>
		<h2>Conclusion</h2>
		<ul>
		  <li class="fragment">We have studied several techniques to improve the performance of Reed-Solomon codes according to their coding mechanism, and figured out the best choices on the basis of GPU architecture.</li>
		  <li class="fragment">We have illustrated methods to reduce the data transfer overhead introduced by the GPU implementation.</li>
		  <li class="fragment">We present an optimized GPU implementation of Reed-Solomon Codes, which can achieve a speedup of 14.71 over the current best CPU implementation.</li>
		</ul>
	  </section>

	  <section>
		<h1>THE END</h1>
		<h3>Q & A</h3>
	  </section>

	</div>

  </div>

  <script src="lib/js/head.min.js"></script>
  <script src="js/reveal.min.js"></script>

  <script>
function slidenumber(event){
  var totalslides = document.querySelectorAll( '.reveal .slides section:not(.stack)' ).length;
  var current_slide = 0;

  var horizontal_slides = document.querySelectorAll( '.reveal .slides>section' );
  for (var i = 0; i < event.indexh; i++) {
	// get subslides
	var subslides = horizontal_slides[i].querySelectorAll('section');

	// if subslides.length is 0, add 1 for horizontal slide, else add subslides.length
	current_slide += (subslides.length === 0) ? 1 : subslides.length;
  }

  current_slide += event.indexv+1;
  return current_slide.toString()+"/"+totalslides.toString();
}

Reveal.addEventListener('slidechanged', function(event){
	document.querySelector(".slidenumber").innerText=slidenumber(event);
	});
Reveal.addEventListener('ready', function(event){
	document.querySelector(".slidenumber").innerText=slidenumber(event);
	});
  </script>

  <script>

// Full list of configuration options available here:
// https://github.com/hakimel/reveal.js#configuration
  Reveal.initialize({
controls: false,
progress: true,
// Display the page number of the current slide
slideNumber: true,
history: true,
center: true,
embedded: true,

theme: Reveal.getQueryHash().theme, // available themes are in /css/theme
transition: Reveal.getQueryHash().transition || 'default', // default/cube/page/concave/zoom/linear/fade/none

//				math: {
//				    mathjax: 'http://cdn.mathjax.org/mathjax/latest/MathJax.js',
//				    config: 'TeX-AMS_HTML-full'  // See http://docs.mathjax.org/en/latest/config-files.html
//				},

// Optional libraries used to extend on reveal.js
dependencies: [
{ src: 'lib/js/classList.js', condition: function() { return !document.body.classList; } },
{ src: 'plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
{ src: 'plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
{ src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
{ src: 'plugin/zoom-js/zoom.js', async: true, condition: function() { return !!document.body.classList; } },
{ src: 'plugin/notes/notes.js', async: true, condition: function() { return !!document.body.classList; } },
{ src: 'plugin/math/math.js', async: true }
]
});

  </script>

  <a href="https://github.com/yszheda/GPU-RSCode"><img style="position: absolute; top: 0; left: 0; border: 0;" src="https://s3.amazonaws.com/github/ribbons/forkme_left_darkblue_121621.png" alt="Fork GPU-RSCode on GitHub"></a>

  </body>
  </html>
